\documentclass{article}

\title{Learning Incoherent Subspaces: Classification via Incoherent Dictionary Learning.}

\author{Daniele Barchiesi and Mark D. Plumbley}
%\affiliation{Centre for Digital Music\\
%	Queen Mary University of London\\
%	Mile End Road, London E1 4NS, UK}

%%%%%%%%%%%%%%%%%%%%%%PACKAGES%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[applemac]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsfonts,amssymb,amsmath,amsthm,bm}
\usepackage[boxruled,linesnumbered]{algorithm2e}
\usepackage{subfigure,graphicx}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{epstopdf}


%%%%%%%%%%%%%%%%%%%%%DEFINITIONS%%%%%%%%%%%%%%%%%%
\input{definitions.tex}	


\def \nComponents{L} 	%number of significant components of ICA feature transform
\def \fea{\Vector{x}} 	%vector of features
\def \Feas{\Matrix{X}} 	%matrix containing set of training features
\def \iFea{n}	 		%component index of features vector
\def \nDim{N} 			%dimensionality of features vector
\def \newFea{\Vector{y}} 	%features vector after feature transform
\def \NewFeas{\Matrix{Y}}%matrix of training features after feature transform
\def \Dic{\Matrix{\Phi}} %dictionary learned from features
\def \nAto{K} 			%number of atoms in the dictionary
\def \iAto{k} 			%atom index
\def \atom{\Vector{\phi}}%atom in a dictionary
\def \nFea{M}			%number of signals or features
\def \iFea{m}			%signal index
\def \Coeff{\Matrix{A}} 	%matrix of sparse approximation coefficients
\def \coherence{\mu} 		%mutual coherence
\def \coeff{\Vector{\alpha}}%vector of sparse approximation coefficients
\def \nActiveAtoms{S} 	%number of active atoms
\def \orthmat{\Matrix{W}} 	%orthonormal matrix
\def \cat{c} 			%category
\def \cats{\Vector{c}}	%vector of categories of training signals
\def \Cat{\mathcal{C}} 	%set of possible categories
\def \uniCat{C} 			%element of the set of possible categories
\def \iCat{p}  		%index of the elements in the set of possible categories
\def \nCat{P} 		%number of elements in the set of possible categories
\def \definition{\overset{\Label{def}}{=}}	%definitions
\def \Gram{\Matrix{G}} 						%Gram matrix
\def \gram{g} 								%element of the Gram matrix
\def \admissibleDictionary{\Function{D}} 	%set of admissible dictionaries
\def \ambient{\Set{R}} 						%ambient
\def \ipr{\Acronym{ipr}} 					%Iterative projections and rotations
\def \ip{\Acronym{ip}}						%Iterative projections
\def \nDimSub{Q}
\def \Spa{\Matrix{\Psi}}

\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
In this article we present the supervised iterative projections and rotations (\Acronym{s-ipr}) algorithm, a method for learning discriminative incoherent subspaces from data. We derive \Acronym{s-ipr} as a supervised extension of our previously developed iterative projections and rotations (\Acronym{ipr}) algorithm for incoherent dictionary learning, and we employ it to learn incoherent sub-spaces that model signals belonging to different classes. We test our method as a feature transform for supervised classification, first by visualising transformed features from a synthetic dataset and from the `iris' dataset, then by using the resulting features in a classification experiment. While the visualisation results are promising, we find that \Acronym{s-ipr} generally performs worse than traditional and state-of-the-art techniques for supervised dimensionality reduction in terms of the misclassification ratio.
\end{abstract}
%
%\begin{keywords}
%Feature transforms, sparse approximation, dictionary learning, supervised classification.
%\end{keywords}
%
\section{Introduction: Classification And Feature Transforms}\label{sec:intro}
Supervised classification is one of the classic problems in machine learning where a system is designed to discriminate the category of an observed signal, having previously observed representative examples from the considered classes \cite{Duda1973Pa}.

Typically, a classification algorithm consists of a training phase where  class-specific models are learned from labelled samples, followed by a testing phase where unlabelled data are classified by comparison with the learned models. Both training and testing comprise various stages. Firstly, we observe a signal that measures a process of interest, such as the recording of a sound or image, or a log of the temperatures in a particular geographic area. Then, a set of features are extracted from the raw signals using signal processing techniques. This step is performed in order to reduce the dimensionality of the data and provide a new signal that allows generalisation among examples of the same class, while retaining enough information to discriminate between different classes. 

Following the features extraction step, a feature transform can be employed to further reduce the dimensionality of the data and to enhance discrimination between classes. Thus classification benefits from feature transforms especially when features are not separable, that is, when it is not possible to optimise a simple function that maps features belonging to signals of a given class to the corresponding category. 
A further dimensionalty reduction may be performed when dealing with high dimensional signals (such as audio or high resolution images) by fitting the parameters of global statistical distributions with features learned on portions of the signal. Models learned on different classes are finally compared using a distance metric to the model learned form an unlabelled signal, which is typically assigned to the nearest class.

\subsection{Traditional Algorithms For Feature Transform}\label{sec:aft}
Two of the main feature transform techniques include principal component analysis (\Acronym{pca}) \cite{Pearson1901On} and Fisher's linear discriminant analysis (\Acronym{lda}) \cite{Duda1973Pa}.

\subsubsection{\Acronym{pca}}

Let $\curlyb{\fea_{\iFea} \in \real^{\nDim}}_{\iFea=1}^{\nFea}$ be a set of vectors containing features extracted from $\nFea$ training signals. The goal of \Acronym{pca} is to learn an orthonormal set of basis functions $\curlyb{\atom_{\iAto} \in \ambient^{\nDim}}_{\iAto=1}^{\nDim}$ such that $ \norm{\atom_{\iAtom}}{2}=1$ and $\inner{\atom_{i}}{\atom_{j}}=0 \; \forall i\neq j$ that are placed along the columns of a so-called \emph{dictionary} $\Dic\in\real^{\nDim\times\nDim}$. The bases are optimised from the data to identify their principal components, that is, the sub-spaces that retain the maximum variance of the features.

To compute the dictionary, the eigenvalue decomposition of the outer product
\begin{equation}
\Matrix{X}\Transpose{\Matrix{X}} = \Matrix{Q}\Matrix{\Lambda}\Transpose{\Matrix{Q}}
\end{equation}
is first calculated. Then, the $\nComponents$ eigenvectors corresponding to the $\nComponents$ largest eigenvalues are selected from the matrix $\Matrix{Q}$, and scaled to unit $\ell_{2}$ norm to form the dictionary $\Dic$. A new set of transformed features $\newFea_{\Acronym{pca}} = \Dic\Transpose{\Dic}\fea$ is computed by projecting the data onto the sub-space spanned by the columns of $\Dic$ (that is, onto the $\nComponents$-dimensional principal sub-space). This operation reduces the dimensionality of the features by projecting them onto a linear subspace embedded in $\real^{\nDim}$. It is an unsupervised technique that does not exploit knowledge about the classes associated with the training set, but implicitly relies in the assumption that the principal component directions encode relevant differences between classes.

\subsubsection{\Acronym{lda}}
In contrast, \Acronym{lda} is a supervised method for feature transform whose objective is to explicitly maximise the separability of classes in the transformed domain. 

Let $\obsSet_{\iUniCat}$ be a set indexing features extracted from data belonging to the $\iUniCat$-th category, let
\begin{equation}
\average{\fea}_{\iUniCat} \definition \frac{1}{\abs{\obsSet_{\iUniCat}}}\sum_{\iObs\in\obsSet_{\iUniCat}} \fea_{\iObs}
\end{equation} 
be the $\iUniCat$-th class feature centroid, and $\average{\fea}\definition\sum_{\iObs=1}^{\nObs}\fea_{\iObs}$ the centroid of the features extracted from the entire training dataset. The between-classes scatter matrix 
\begin{equation}
\Matrix{S_{b}} \definition \sum_{\iUniCat=1}^{\nUniCat}\abs{\obsSet_{\iUniCat}}\roundb{\average{\fea}_{\iUniCat}-\average{\fea}}\Transpose{\roundb{\average{\fea}_{\iUniCat}-\average{\fea}}}
\end{equation} 
is defined to measure the mutual distances between the centroids of different classes, while the within-classes scatter matrix 
\begin{equation}
\Matrix{S}_{w}\definition \sum_{\iUniCat=1}^{\nUniCat}\sum_{\iObs\in\obsSet_{\iUniCat}}\roundb{\fea_{\iObs}-\average{\fea}_{\iUniCat}}\Transpose{\roundb{\fea_{\iObs}-\average{\fea}_{\iUniCat}}}
\end{equation}
quantifies the distances between features belonging to the same class.
To maximise an objective function $\objective(\Matrix{W})\definition\frac{\abs{\Transpose{\Matrix{W}}\Matrix{S}_{b}\Matrix{W}}}{\abs{\Transpose{\Matrix{W}}\Matrix{S}_{w}\Matrix{W}}}$ that promotes features belonging to the same class to be near each other and far away from features belonging to other classes, the eigenvalue decomposition of the matrix
\begin{equation}
\pseudoinverse{\Matrix{S}_{w}}\Matrix{S}_{b} = \Matrix{Q}\Matrix{\Lambda}\Transpose{\Matrix{Q}}
\end{equation}
is computed, and the features $\fea$ are projected onto the space spanned by its $(\nUniCat-1)$ eigenvectors corresponding to the largest $(\nUniCat-1)$ eigenvalues.

\Acronym{lda} explicitly seeks to enhance the discriminative power of features by optimising the objective $\objective$.
\subsection{Supervised \Acronym{pca}}
Related works that extend \Acronym{pca} include the supervised \Acronym{pca} (\Acronym{s-pca}) proposed by Barshan et al. \cite{Barshan2011Su}. \Acronym{s-pca} is based on the theory of reproducing kernel Hilbert spaces (\Acronym{rkhs}) (that are spaces of functions which satisfy certain properties and map elements from an arbitrary set to the set of complex numbers) \cite{Aronszajn:1950}, and on the so-called Hilbert-Schmidt independence criterion (\Acronym{hsic})\cite{gretton2005measuring}. The \Acronym{hsic} is used to estimate the statistical dependence of two random variables based on the fact that this quantity is related to the correlation of functions belonging to their respective \Acronym{rkhs}. While \Acronym{hsic} is defined in terms of the probability density function of the two random variables,  empirical estimates of \Acronym{hsic} can be obtained from finite sequences of their realisations. The empirical \Acronym{hsic} can be used in turn to construct an objective function that maximises the dependence between the two variables. Hence, this strategy is adopted within the context of classification to maximise the statistical dependence between a transformed feature $\newFea_{\Acronym{s-pca}}$ and its corresponding category $\cat$.

In practice, \Acronym{s-pca} differs from \Acronym{pca} in that it calculates the eigenvalue decomposition of a matrix $\Matrix{R}$ defined as follows:
\begin{equation}
\Matrix{R} \definition \Matrix{X}\Matrix{H}\Matrix{L}\Matrix{H}\Transpose{\Matrix{X}}
\end{equation}
were $\Matrix{H} \definition \Matrix{I} - \Vector{e}\Transpose{\Vector{e}}$ is a so-called \emph{centring} matrix\footnote{Here $\Vector{e}$ is a vector of ones.} and $\Matrix{L} \definition \Vector{\cat}\Transpose{\Vector{\cat}}$ is the kernel matrix of the class variable that is constructed by computing the outer product of the vectors resulting from assigning different numerical values to each category. 

\subsection{Other related work}
The union of incoherent sub-spaces model proposed by Schnass and Vandergheynst \cite{Schnass2010A-} employes a very similar intuition to the one that inspired our proposed method, and models features belonging to different classes using incoherent subspaces. 

Other methods for supervised dimensionality reduction include metric learning algorithms \cite{xing2002distance}, sufficient dimensionality reduction \cite{li1991sliced} and Bair's supervised principal components \cite{Bair06predictionby}.

Manifold learning techniques are used to model nonlinear data and reviewed by Van Der Maaten et al. \cite{Van-Der-Maaten2009Di}. Finally,  the sparse sub-space clustering technique developed by Elhamifar and Vidal \cite{Elhamifar2013Sp} that applies concepts and algorithm from the field of sparse approximation to tackle unsupervised clustering problems.

\subsection{Paper organisation}
The method proposed in this paper is aimed at learning discriminative sub-spaces that allow dimensionality reduction, while at the same time enhancing the separability between classes. It is derived from our previous work on learning incoherent dictionaries for sparse approximation \cite{Barchiesi2013Le}.

The incoherent dictionary learning problem will be introduced in Section \ref{sec:idl}, while Section \ref{sec:lis} will contain the main contribution of this paper consisting in learning incoherent subspaces for classification. Numerical experiments are presented in Section \ref{sec:ne}, and conclusions are drawn in Section \ref{sec:end}.

\section{Incoherent Dictionary Learning}\label{sec:idl}
A sparse approximation of a signal $\fea\in\real^{\nDim}$ is a linear combination of $\nAto\geq\nDim$ basis functions $\curlyb{\atom_{\iAto}\in\real^{\nDim}}_{\iAto=1}^{\nAto}$ called \emph{atoms} described by:
\begin{equation}
	\fea \approx \approximant{\fea} = \sum_{\iAto=1}^{\nAto} \alpha_{\iAto}\atom_{\iAto}
\end{equation}
where the vector of coefficients $\coeff$ contains a \emph{small} number of non-zero components, corresponding to a small number of atoms actively contributing to the approximation $\approximant{\fea}$. Given a signal $\fea$ and a dictionary, various algorithms have been proposed to find a sparse approximation that minimises the residual error $\norm{\fea-\approximant{\fea}}{2}$\cite{Elad2010Sp}.

Dictionary learning aims at optimising a dictionary $\Dic$ for sparse approximation given a set of training data. It is an unsupervised technique that can be thought as being a generalisation of \Acronym{pca}, as both methods learn linear subspaces that minimise the approximation error of the signals. Dictionary learning, however, is generally more flexible than \Acronym{pca} because it can be employed to learn more general non-orthogonal over-complete dictionaries \cite{Rubinstein2010Di}.

\subsection{The incoherent dictionary learning problem}
Dictionaries for sparse approximation have important intrinsic properties that describe the relations between their atoms, like the mutual coherence $\mu(\Dic)=\underset{i\neq j}{\max}{\inner{\atom_{i}}{\atom_{j}}}$ that is defined as the maximum inner product between any two different atoms. The goal of incoherent dictionary learning is to learn atoms that are well adapted to sparsely approximate a set of training signals, and that are at the same time mutually incoherent \cite{Barchiesi2013Le}. 

Given a set of $\nFea$ training signals contained in the columns of the matrix $\Feas \in \real^{\nDim\times\nFea}$ and a matrix $\Coeff\in\real^{\nAto\times\nFea}$ indicating the sparse approximation coefficients, the incoherent dictionary learning problem can be expressed as:

\begin{align}\label{eq:iprcost}
\optimal{\Dic} = \MinimiseST{\Dic}{\norm{\Feas-\Dic\Coeff}{\F}}{\coherence(\Dic) \leq \coherence_{0} \nonumber \\
			&\norm{\coeff_{\iFea}}{0}\leq \nActiveAtoms \quad \forall \iFea}
\end{align}
where $\coherence_{0}$ is a fixed mutual coherence constraint, the $\ell_{0}$ pseudo-norm $\norm{\cdot}{0}$ counts the number of non-zero components of its argument and $\nActiveAtoms$ is a fixed number of active atoms. Algorithms for (incoherent) dictionary learning generally follow an alternate optimisation heuristic, iteratively updating $\Dic$ and $\Coeff$ until a stopping criterion is met. In the case of the iterative projections and rotations algorithm (\Acronym{ipr}) algorithm \cite{Barchiesi2013Le}, a dictionary de-correlation step is added after updating the dictionary in order to satisfy the mutual coherence constraint. 

Given $\Feas$, fixed $\coherence_{0}$, $\nActiveAtoms$ and a stopping criterion (such as a maximum number of iterations), the optimisation of \eqref{eq:iprcost} is tackled by iteratively performing the following steps:
\begin{itemize}
	\item\emph{Sparse coding}: fix $\Dic$ and compute the matrix $\Coeff$ using a suitable sparse approximation method.
	\item\emph{Dictionary update}: fix $\Coeff$ and update $\Dic$ using a suitable method for dictionary learning.
	\item\emph{Dictionary de-correlation}: given $\Feas$, $\Dic$ and $\Coeff$ update the dictionary $\Dic$ to reduce its mutual coherence under the level $\coherence_{0}$.
\end{itemize}

\subsection{The iterative projections and rotations algorithm}\label{sec:ipr}
The \Acronym{ipr} algorithm has been proposed in order to solve the dictionary de-correlation step, while ensuring that the updated dictionary provides a sparse approximation with low residual norm, as indicated by the objective function \eqref{eq:iprcost} \cite{Barchiesi2013Le}.

The \Acronym{ipr} algorithm requires the calculation of the Gram matrix $\Gram=\Transpose{\Dic}\Dic$ which contains the inner products between any two atoms in the dictionary. $\Gram$ is iteratively projected onto two constraint sets, namely the structural constraint set $\stcset$ and the spectral constraint set $\spcset$. The former is the set of symmetric square matrices with unit diagonal values and off-diagonal values with magnitude smaller or equal than $\coherence_{0}$:
\small
\begin{equation*}
	\stcset \definition \curlyb{\stcmat \in \ambient^{\nAtoms \times \nAtoms} : \stcmat = \Transpose{\stcmat}, \stcel_{i,i}=1,\max_{i > j}|\stcel_{i,j}|\leq \coherence_{0}}.
\end{equation*}
\normalsize
The latter is the set of symmetric positive semidefinite square matrices with rank smaller than or equal to $\nDimensions$:
\begin{equation*}
\spcset \definition \curlyb{ \spcmat \in \ambient^{\nAtoms \times \nAtoms} : \spcmat = \Transpose{\spcmat}, \operatorname{eig}(\spcmat)\geq \Vector{0}, \operatorname{rank}(\spcmat)\leq \nDimensions}
\end{equation*}  
where the operator $\operatorname{eig}(\cdot)$ returns the vector of eigenvalues of its argument.

Starting from the Gram matrix of an initial dictionary $\Dictionary$, the \Acronym{ipr} method iteratively performs the following operations.
\begin{itemize}
\item \emph{Projection onto the structural constraint set}. The projection $\stcmat = \Projection_{\stcset}(\Gram)$ can be obtained by:
\begin{enumerate}
\item setting $\stcel_{i,i} = 1$,
\item limiting the off-diagonal elements so that, for $i \neq j$, 
\small
\begin{equation}\label{eq:pscs}
\stcel_{i,j} = \operatorname{Limit}({\gram}_{i,j},\coherence_{0}) = \left\{ \begin{array}{rl}
	\gram_{i,j}  & \text{if} \quad |\gram_{i,j}|\leq \coherence_{0} \\
	\operatorname{sgn}(\gram_{i,j})\coherence_{0}  & \text{if} \quad |\gram_{i,j}| > \coherence_{0}
\end{array} \right.
\end{equation}
\normalsize
\end{enumerate}
\item \emph{Projection onto the spectral constraint set and factorization}. The projection $\spcmat = \Projection_{\spcset}(\Gram)$ and subsequent factorisation are obtained by:
\begin{enumerate}
\item calculating the eigenvalue decomposition (\Acronym{evd}) $\Gram = \eigvecmat \eigvalmat \Transpose{\eigvecmat}$,
\item thresholding the eigenvalues by keeping only the $\nDimensions$ largest positive ones.
\begin{equation*}
	\left[\operatorname{Thresh}(\eigvalmat,\nDimensions) \right]_{i,i} = \left\{ \begin{array}{rl}
	\lambda_{i,i}  & \text{if} \quad i \leq N \; \text{and} \; \lambda_{i,i}>0 \\
	0  & \text{if} \quad i > N \; \text{or} \; \lambda_{i,i}\leq 0
	\end{array}\right.
\end{equation*}
where the eigenvalues in $\eigvalmat$ are ordered from the largest to the smallest. Following this step, at most $\nDimensions$ eigenvalues of the Gram matrix are different from zero,
\item factorizing the projected Gram matrix into the product $\Gram=\Transpose{\Dic}\Dic$ by setting:
\begin{equation}
\Dic = \eigvalmat^{1/2}\Transpose{\eigvecmat}.
\end{equation}
\end{enumerate}
\item \emph{Dictionary rotation}. Rotate the dictionary $\Dic$ to align it to the training set by solving the problem:
\begin{equation}\label{eq:rot}
	\optimal{\orthmat} = \Minimise{\orthmat \Transpose{\orthmat} = \Matrix{I}}{\norm{\Feas - \orthmat\Dic\Coeff}{\Label{F}}}.
\end{equation}
The optimal rotation matrix can be calculated by:
\begin{enumerate}
\item computing the sample covariance between the observed signals and their approximations $\CovMat \definition (\Dic\Coeff)\Transpose{\Feas}$,
\item calculating the \Acronym{svd} of the covariance $\CovMat = \Matrix{U}\Matrix{\Sigma}\Transpose{\Matrix{V}}$,
\item setting the optimal rotation matrix to $\optimal{\orthmat}=\Matrix{V}\Transpose{\Matrix{U}}$,
\item rotating the dictionary $\Dic \leftarrow \optimal{\orthmat}\Dic$.
\end{enumerate}
\end{itemize}

More details about the \Acronym{ipr} algorithm can be found in  \cite{Barchiesi2013Le}, including details of its computational cost.
%The code of the \Acronym{ipr} method is illustrated in Algorithm \ref{algo:ipr}.
%
%\begin{algo}
%\KwIn{$\Feas, \Dictionary, \Coeff, \coherence_{0}, \nIter$}
%\KwOut{$\optimal{\Dictionary}$}
%$\iter\gets1$\;
%\While{$\iter \leq \nIter$ and $\coherence(\Dictionary)>\coherence_{0}$}{
%	\tcp{Calculate Gram matrix}
%	$\Gram \gets \Transpose{\Dictionary}\Dictionary$\;
%	\tcp{Project ont structural c.s.}
%	$\operatorname{diag(\Gram)} \gets \Vector{1}$\;
%	$\Gram \gets \operatorname{Limit}(\Gram,\coherence_{0})$\;
%	\tcp{Project Gram matrix onto spectral c.s. and factorize}
%	$[\eigvecmat, \eigvalmat] \gets \Acronym{evd}(\Gram)$\;
%	$\eigvalmat \gets \operatorname{Thresh}(\eigvalmat,\nDimensions)$\;
%	$\Dictionary \gets \eigvalmat^{1/2}\Transpose{\eigvecmat}$\;
%	\tcp{Rotate dictionary}
%	$\CovMat \gets \Feas\Transpose{(\Dictionary\Coeff)}$\;
%	$[\Matrix{U},\Matrix{\Sigma},\Matrix{V}] \gets \Acronym{svd}(\CovMat)$\label{algo:ipr:svd}\;
%	$\OrthMat \gets \Matrix{V}\Transpose{\Matrix{U}}$\;
%	$\Dictionary \gets \OrthMat\Dictionary$\;
%	$\iter\gets\iter+1$\;
%}
%\caption{\label{algo:ipr}Iterative projections and rotations (\Acronym{ipr})}
%\end{algo}


\section{Learning Incoherent Subspaces}\label{sec:lis}
The \Acronym{ipr} algorithm learns a dictionary where all the atoms are mutually incoherent. Therefore, given any two disjoint sets $\Lambda\bigcap\Gamma=\emptyset$ that identify non-overlapping collections of atoms, the sub-dictionaries $\Dic_{\Lambda}, \Dic_{\Gamma}$ are also mutually incoherent.

Starting from this observation, the main intuition driving the development of a supervised \Acronym{ipr} (\Acronym{s-ipr}) algorithm for classification is to learn mutually incoherent sub-dictionaries that approximate features from different classes of signals. The sub-dictionaries are in turn used to define incoherent sub-spaces, and features are projected onto these sub-spaces yielding discriminative dimensionality reduction. 
  
\subsection{The supervised \Acronym{ipr} algorithm}\label{sec:iprclass}
Let $\curlyb{\cat_{\iFea}\in\Cat}_{\iFea=1}^{\nFea},\; \Cat=\curlyb{\uniCat_{1},\uniCat_{2},\dots,\uniCat_{\nCat}}$ be a set of labels that identify the category of the vectors of features $\fea_{\iFea}$, whose elements belong to a set $\Cat$ of $\nCat$ possible categories. The columns of the matrix $\Feas_{\iCat}$ contain a selection of the features extracted from signals belonging to the $\iCat$-th category.

To learn incoherent sub-dictionaries from the entire set of features, we must first cluster the atoms to different classes\footnote{Note that the term \emph{cluster} implies that a this stage the algorithm needs to make an unsupervised decision, since there is no any a-priori reason to assign a given atom to any particular class.}, and then only proceed with their de-correlation  if they are assigned to different categories (while allowing coherent atoms to approximate features from the same class). To this aim, we employ the matrix $\Coeff$ to measure the contribution of every atom to the approximation of features belonging to each class.

Let $\coeff_{\iCat}^{\iAto}$ indicate the $\iAto$-th row of the matrix $\Coeff_{\iCat}$ containing the coefficients that contribute to the approximation of $\Feas_{\iCat}$, and $\nDim_{\iCat}$ indicate the number of its elements. A coefficient $\gamma_{\iAto,\iCat}$ is defined as:
\begin{equation}
	\gamma_{\iAto,\iCat} \definition \frac{1}{\nDim_{\iCat}}\norm{\coeff_{\iCat}^{\iAto}}{1},
\end{equation}
and every atom $\atom_{\iAtom}$ is associated with the category to which it maximally contributes $\optimal{\iCat}_{\iAto} = \underset{\iCat}{\arg\max}\curlyb{\gamma_{\iAto,\iCat}}$. 

Grouping together atoms that have been assigned to the same class leads to a set of sub-dictionaries whose size and rank depends on the number of atoms for each class, and to their linear dependence. As a general heuristic, if  features corresponding to different classes do not occupy the same sub-space (according to the active elements in $\Coeff$), a full-rank dictionary $\Dic$ with $\nAto \geq \nDim \gg \nCat$ ensures that $\optimal{\iCat}_{\iAto}$ identify $\nCat$ non-empty and disjoint sub-dictionaries $\curlyb{\Dic_{\iCat}}_{\iCat=1}^{\nCat}$.

Once the atoms have been clustered, the Gram matrix $\Gram$ is computed and iteratively projected as in the method described in Section \ref{sec:ipr}, with the difference that equation \eqref{eq:pscs} is modified in order to only constraint the mutual coherence between atoms assigned to different categories
\small
\begin{equation}\label{eq:pscs2}
\operatorname{Limit}({\gram}_{i,j},\coherence_{0},\optimal{\Vector{\iCat}}) = \left\{ \begin{array}{rl}
	\gram_{i,j}  & \text{if} \quad |\gram_{i,j}|\leq \coherence_{0} \, \text{or} \, \optimal{\iCat}_{i}=\optimal{\iCat}_{j} \\
	\operatorname{sgn}(\gram_{i,j}) \coherence_{0}  & \text{if} \quad |\gram_{i,j}|   >  \coherence_{0} \, \text{and} \, \optimal{\iCat}_{i}\neq\optimal{\iCat}_{j}
\end{array} \right.
\end{equation}
\normalsize


A further modification of the standard \Acronym{ipr} algorithm presented in \cite{Barchiesi2013Le} consists in the update of the Gram matrix, performed by computing its element-wise average with the projection $\stcmat = \Projection_{\stcset}(\Gram)$ (rather than by using the projection alone). This heuristic has led to improved empirical results by preventing $\Gram$ from changing too abruptly. 

The complete supervised \Acronym{s-ipr} method is summarised in Algorithm \ref{algo:sipr}. Note that the mutual coherence $\coherence_{\optimal{\iCat}}(\Dic) = \underset{\optimal{\iCat}_{i}\neq\optimal{\iCat}_{j}}{\arg\max}\inner{\atom_{i}}{\atom_{j}}$ indicated in this algorithm measures the inner product between any two atoms assigned to different categories since atoms assigned to the same category are allowed to be mutually coherent.
\begin{algo}
\KwIn{$\Feas, \Dictionary, \Coeff, \coherence_{0}, \cats, \nIter$}
\KwOut{$\optimal{\Dictionary}$}
$\iter\gets1$\;
\tcp{Cluster atoms}
$\Coeff_{\iCat} \gets \squareb{\coeff_{j}} \forall j \in \uniCat_{\iCat}$\;
$\gamma_{\iAto,\iCat} \gets \norm{\coeff_{\iCat}^{\iAto}}{1}/\nDim_{\iCat}$\;
$\optimal{\iCat}_{\iAto} = \underset{\iCat}{\arg\max}\curlyb{\gamma_{\iAto,\iCat}}$\;
\While{$\iter \leq \nIter$ and $\coherence_{\optimal{\iCat}}(\Dictionary)>\coherence_{0}$}{
	\tcp{Calculate Gram matrix}
	$\Gram \gets \Transpose{\Dictionary}\Dictionary$\;
	\tcp{Project onto structural c.s.}
	$\operatorname{diag}(\stcmat) \gets \Vector{1}$\;
	$\stcmat \gets \operatorname{Limit}(\Gram,\coherence_{0},\optimal{\Vector{\iCat}})$\;
	$\Gram \gets \frac{1}{2}\Gram + \frac{1}{2}\stcmat$\;
	\tcp{Project onto spectral c.s. and factorize}
	$[\eigvecmat, \eigvalmat] \gets \Acronym{evd}(\Gram)$\;
	$\eigvalmat \gets \operatorname{Thresh}(\eigvalmat,\nDimensions)$\;
	$\Dictionary \gets \eigvalmat^{1/2}\Transpose{\eigvecmat}$\;
	\tcp{Rotate dictionary}
	$\CovMat \gets \Feas\Transpose{(\Dictionary\Coeff)}$\;
	$[\Matrix{U},\Matrix{\Sigma},\Matrix{V}] \gets \Acronym{svd}(\CovMat)$\label{algo:ipr:svd}\;
	$\OrthMat \gets \Matrix{V}\Transpose{\Matrix{U}}$\;
	$\Dictionary \gets \OrthMat\Dictionary$\;
	$\iter\gets\iter+1$\;
}
\caption{\label{algo:sipr}Supervised \Acronym{ipr}}
\end{algo}
\subsection{Classification via incoherent subspaces}\label{sec:class}
The \Acronym{s-ipr} algorithm allows to learn a set of sub-dictionaries $\curlyb{\Dic_{\iCat}}$ that contain mutually incoherent atoms. These cannot be directly used to define discriminative subspaces because, depending on $\nDim$ and on the rank of each sub-dictionary, atoms belonging to disjoint sub-dictionaries might span identical subspaces. Instead, we fix a rank $\nDimSub\leq\floor{\nDim/\nCat}$ and choose a collection of $\nDimSub$ linearly independent atoms from each sub-dictionary $\Dic_{\iCat}$, using the largest values of $\gamma_{\iAto,\iCat}$ to define a picking order. Thus, we obtain a set $\curlyb{\Spa_{\iCat}}_{\iCat=1}^{\nCat}$ of incoherent sub-spaces of rank $\nDimSub$ embedded in the space $\ambient^{\nDim}$, and use them to derive a feature transform for classification.
 
Each feature vector $\fea_{\iFea}$ that belongs to the class $\cat_{\iFea}$ is projected onto the relative subspace, yielding a set of transformed features $\curlyb{\newFea_{\iFea}}_{\iFea=1}^{\nFea}$.
\begin{equation}
\newFea_{\iFea} = \Spa_{\cat_{\iFea}}\pseudoinverse{\Spa_{\cat_{\iFea}}}\fea_{\iFea}
\end{equation}
where $\Spa^{\dagger}$ denotes the Moore-Penrose pseudo-inverse of the matrix $\Spa$ and needs to be used in place of the transposition operator because the columns of $\Spa$ are in general not orthogonal.

When an unlabelled signal is presented to the classifier, the corresponding vector of features $\fea$ is projected onto all the learned sub-spaces. Then, the nearest sub-space is chosen using an Euclidean distance measure, and the corresponding projection $\newFea$ used as the transformed feature.
\begin{align}
	\optimal{\iCat} &= \underset{\iCat}{\arg\min}\norm{\fea-\Spa_{\iCat}\pseudoinverse{\Spa_{\iCat}}\fea}{2}	\\
	\newFea &= \Spa_{\optimal{\iCat}}\pseudoinverse{\Spa_{\optimal{\iCat}}}\fea
\end{align}
The subspace $\optimal{\iCat}$ can be directly used as an estimator of the category of the signal $\optimal{\cat}$. Alternatively, a simple \emph{k-neaerst neighbour} classifier can be employed on the transformed features, and a class can be inferred as:
\begin{equation}
\optimal{\cat} = \texttt{knn}(\newFea,\NewFeas,\cats)
\end{equation}
where $\NewFeas$ represents the matrix of training features after the transform stage. This latter approach is especially suitable when working with a large number of classes in a space of relatively small dimension, as in this case multiple classes might be assigned to the same subspace.
\section{Numerical Experiments}\label{sec:ne}
\subsection{Feature visualisation}\label{sec:visu}
To illustrate the \Acronym{s-ipr} algorithm for feature transform, we first run visualisation experiments depicting how different feature transform methods act on training and test data.
\subsubsection{Synthetic data}
\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{./Code/Datasets/GetToyExampleDataset.pdf}
\caption{\label{fig:toy}Synthetic data generated along one-dimensional subspaces of $\real^{2}$.}
\end{figure}
Figure \ref{fig:toy} displays a total of $1500$ synthetic features in $\real^{2}$ belonging to $3$ different classes that we generated for this experiment. For each class, first we draw values distributed uniformly in the interval $\squareb{-1,1}$ and assign them to the first component of the features (the \emph{x} coordinate). Then, we add Gaussian noise with variance $0.1$ to the second component (the \emph{y} coordinate), and we rotate the resulting data by the angles $\theta_{0}=0$, $\theta_{1}=\pi/4$ and $\theta_{3}=\pi/2$ for the $3$ classes respectively. This way, features belonging to different classes are clustered along different one-dimensional sub-spaces of $\real^{2}$.
\begin{figure}
\includegraphics[width=\textwidth]{./Code/toyvisu.pdf}
\caption{\label{fig:toyNewFea}Feature transform applied to the synthetic data in Figure \ref{fig:toy}. Different colours correspond to different classes, `+' and `o' markers represent samples taken from the training and test set respectively.}
\end{figure}

Figure \ref{fig:toyNewFea} displays the result of the application of feature transforms to the data depicted in Figure \ref{fig:toy} using subspaces of dimension $1$ (with the exception of \Acronym{lda} that projects the data onto a space of dimension $\nUniCat-1=2$). To generate the plots, we divided the data into a training set (displayed using the `+' marker) and a test set (displayed using the `o' marker). Samples were drawn in random order from the dataset and assigned to either the training set or the test set, with the former containing $70\%$ of the total data and the latter containing the remaining $30\%$. Then, we applied feature transforms on the training set, thereby learning the transform operators, and applied them to the test set. 

Starting from the top-left plot, we can observe that \Acronym{PCA} identified the direction $x=y$ as the one-dimensional subspace that contains most of the variance of the training set. However, given the type of dataset and the dimensionality reduction caused by \Acronym{pca}, features from all classes are overlapping, making this transform a poor choice for classification. Similar observations can be drawn from analysing the result of \Acronym{s-pca}, although this transform identifies the direction $y=0$ as the one that leads to statistical dependence between the value of the transformed features in the training set and the relative class. \Acronym{lda} does not introduce any dimensionality reduction in this case, as it projects the features onto a space of dimension $\nUniCat-1=2$, leaving the original features unaltered. However, in the \Acronym{lda} plot we can appreciate the separation between training set and test set that is difficult to notice in the other plots. 

Finally, the plot at the right-bottom corner of Figure \ref{fig:toyNewFea} displays the results of the \Acronym{s-ipr} algorithm. In setting the parameters of \Acronym{s-ipr}, we chose a $2$ times over-complete dictionary, a number of active atoms equal to half the dimension of the data, and minimal mutual coherence. In the case considered here, this means $\nAto = 4$, $\nActiveAtoms = 1$ and $\coherence = \sqrt{(\nAto-\nDim)/\nDim(\nAto-1)} \approx 0.33$. As discussed in Section \ref{sec:iprclass}, \Acronym{s-ipr} does not project whole sets of features onto a unique sub-space, but rather learns one sub-space for each category, and projects features onto the nearest sub-space. The result depicted here shows that three directions were identified containing data from mostly one category each. Since the incoherent dictionary learning is designed to learn atoms with minimal mutual coherence, the angles between the directions of the sub-spaces learned by \Acronym{s-ipr} are approximately equal. Prior information regarding the directions of the data would allow to relax the parameter $\coherence$, and track more closely the directions of the three data classes.

\subsubsection{Iris dataset}
\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{./Code/Datasets/GetFisherIrisDataset.pdf}
\caption{\label{fig:fisher3} First three features of the `iris' dataset depicting measurements of sepal length, sepal width and petal length of three iris species.}
\end{figure}
Figure \ref{fig:fisher3} displays a subset of the `iris' dataset, a popular database that has been used extensively to test and benchmark classification algorithms. The original dataset contains measurements of the sepal length, sepal width, petal length and petal width of three species of iris, namely `setosa', `versicolor' and `virginica'. In this visualisation experiment we selected the first $3$ features to be able to depict the data using three dimensional scatter plots. From observing the distribution of the data in the feature space, we see that `setosa' is relatively separated from the other two classes, while the features relative to `virginica' and `versicolor' substantially overlap, with only a few exemplars of `virginica' being distinguishable due to large sepal length and petal length.
\begin{figure}
\includegraphics[width=\textwidth]{./Code/fishervisu.eps}
\caption{\label{fig:fisher3NewFea}Feature transform applied to the iris data in Figure \ref{fig:fisher3}. Different colours correspond to different classes, `+' and `o' markers represent samples taken from the training and test set respectively.}
\end{figure}

The results of feature transforms are depicted in Figure \ref{fig:fisher3NewFea}. This time, we learn $2$ dimensional subspaces from the $3$ dimensional data points and plot the transformed features, along with the learned planes. We observe that \Acronym{pca} identifies a direction along a diagonal axis that follows the distribution of features displayed in Figure \ref{fig:fisher3}. \Acronym{s-pca}
, on the other hand, projects the features onto a horizontal plane that slightly enhances the separation between `versicolor' and `virginica' samples. \Acronym{lda} results in a projection where features belonging to the same category are closely clustered together, but fails to separate the classes `versicolor' and `virginica'. Finally, the output of \Acronym{s-ipr} displays three distinct sub-spaces associated with the three classes. As in the other plots, the separation between `versicolor' and `virginica' is far from perfect, however features from the  two classes are mostly projected onto the respective sub-spaces. Features belonging to the `setosa' category are mostly clustered together as a result of their projection onto the black subspace, however we can note a few test samples that have been associated by the algorithm to the blue sub-space.
\subsection{Classification}\label{sec:cla}
\begin{table}
\centering
\begin{tabular}{lccc}\textbf{Name} & \nDim & \nUniCat & \nFea \\
\hline 
Iris & 4 & 3 & 150\\
Balance & 4 & 3 & 625\\
Parkinsons & 23 & 2 & 197 \\
Sonar & 60 & 2 & 208\\
USPS & 256 & 3 & 1405
\end{tabular}
\caption{\label{tab:datasets}Dataset used in the classification evaluation of feature transform algorithms. All the datasets can be downloaded from \href{http://archive.ics.uci.edu/ml/datasets.html}{http://archive.ics.uci.edu/ml/datasets.html}. Note that we only use a subset of the USPS dataset containing the digits $1$, $3$ and $8$.}
\end{table}

In the previous section, we have illustrated how the \Acronym{s-ipr} algorithm is able to learn incoherent sub-spaces that model the distribution of features belonging to different classes. Here we evaluate \Acronym{s-ipr} and the other feature transform algorithms in the context of supervised classification. To perform the classification, features are transformed using the methods already used for comparison in Section \ref{sec:visu} by learning a transform operator on the training set and applying it to the test set. We use a $5$-fold stratified cross-validation to classify all the features in a dataset during the test stage. This method produces $5$ independent classification problems with a ratio between the number of training and test samples equal to $8:2$. Once the features have been transformed, a $k$-nearest neighbour classifier with $k=5$ is used to estimate a class. 

We employ the datasets detailed in Table \ref{tab:datasets}, and for each of them we evaluate the misclassification ratio, that is defined as the fraction of misclassified samples as a proportion of the total number of samples in the test set, averaged over the $5$ independent classification problems created by the stratified cross-validation protocol.
\begin{figure}
\centering
\subfigure{\includegraphics[width=.45\textwidth]{Code/Util/fisheriris.pdf}}
\subfigure{\includegraphics[width=.45\textwidth]{Code/Util/balance.pdf}}
\subfigure{\includegraphics[width=.45\textwidth]{Code/Util/parkinsons.pdf}}
\subfigure{\includegraphics[width=.45\textwidth]{Code/Util/sonar.pdf}}
%\subfigure{\includegraphics[width=.45\textwidth]{Code/Util/usps.pdf}}
\caption{\label{fig:class} Classification results.}
\end{figure}

Figure \ref{fig:class} displays for each dataset the misclassification ratio as a function of the rank of the subspace learned by the algorithms. In the plots `none' indicates that no feature transform was applied (hence resulting in a sub-space rank equal to the dimension of the original features). In general we can see that \Acronym{s-ipr} does not perform as well as the other techniques, and is only comparable at high ranks that do not achieve an overall better classification ratio. Starting from the `iris' dataset, \Acronym{lda} achieves the best performance followed by one-dimensional subspaces learned using \Acronym{pca}. Both \Acronym{s-pca} and \Acronym{s-ipr} work better when learning subspaces of high rank. Note that, at rank $\nDim=4$ all the methods are equivalent because they are not performing dimensionality reduction. The results relative to the balance dataset are similar, with again \Acronym{lda} achieving the best misclassification ratio. Although the results on the `Parkinsons' and `sonar' datasets present similar trends regarding \Acronym{s-ipr}, here \Acronym{lda} does not prove to be as successful as \Acronym{pca} and \Acronym{s-pca} in separating features belonging to different classes. 

\section{Conclusion}\label{sec:end}
\subsection{Summary}
We have presented the \Acronym{s-ipr} algorithm for learning incoherent subspaces from data belonging to different categories. The encouraging experimental results obtained on the visualisation of the synthetic dataset and of a subset of features taken from the 'iris` dataset motivated us to test \Acronym{s-ipr} as a general method for feature transform to be used in classification problems. Unfortunately, we found that the performance of our proposed method on a group of datasets commonly used to benchmark classification methods is not competitive compared to traditional and state-of-the-art methods for feature transform. 

The negative results presented in Section \ref{sec:cla} do not imply that \Acronym{s-ipr} is completely unsuitable as a tool for modelling data for classification, but they rather open a few important areas of future research that should be investigate to better understand the strengths and limitations of the proposed method.

\subsection{Future work}
The main assumption made when using incoherent dictionary learning for classification is that high dimensional features are arranged onto lower-dimensional sub-spaces, and that features belonging to different classes can be modelled using different subspaces that are mutually incoherent. This assumption might be met by some datasets, but might not generally be satisfied by others. Understanding the general distribution of the features in a  dataset might be a necessary first step to inform a subsequent choice of algorithm, so that \Acronym{s-ipr} is only used in cases where its premise about the feature distribution is valid. This same argument holds for the whole class of linear models that comprises the dictionary learning model. Indeed, many feature transform techniques have equivalent kernelized versions to model non-linear data.

Other substantial improvements can be made on the algorithm itself. The present implementation of \Acronym{s-ipr} contains a fixed parameter $\coherence$ that promotes minimal mutual coherence between the sub-spaces used to approximate different data classes. Knowledge about the distribution of the features might lead to relaxing this parameter, learning sub-spaces that are closer to the true distribution of the features and in turn improving class separation. Moreover, different values of mutual coherence for different pairs of subspaces can be easily included in the optimisation, greatly enhancing the flexibility of \Acronym{s-ipr} as a modelling tool.

\bibliographystyle{IEEEbib}
\bibliography{bibliography.bib}
\end{document}
